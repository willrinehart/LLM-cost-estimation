{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1c1fed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DIRECTORY FILE LISTING\n",
      "============================================================\n",
      "Current directory: /Users/yams/Documents/LLM-cost-estimation/AB-1018\n",
      "\n",
      "ALL FILES IN DIRECTORY:\n",
      "----------------------------------------\n",
      " 1. .ipynb_checkpoints\n",
      " 2. Cost compliance estimates fo.textClipping\n",
      " 3. Cost_compliance_estimates_for_AB-1018_ChatGPT_AB-1018_classification.csv\n",
      " 4. Cost_compliance_estimates_for_AB-1018_Claude_AB-1018_classification.csv\n",
      " 5. Cost_compliance_estimates_for_AB-1018_Gemini_AB-1018_classification.csv\n",
      " 6. Cost_compliance_estimates_for_AB-1018_Grok_AB-1018_classification.csv\n",
      " 7. Untitled.ipynb\n",
      " 8. analysis.ipynb\n",
      "\n",
      "Total files: 8\n",
      "\n",
      "============================================================\n",
      "CSV FILES ONLY:\n",
      "----------------------------------------\n",
      " 1. 'Cost_compliance_estimates_for_AB-1018_ChatGPT_AB-1018_classification.csv'\n",
      " 2. 'Cost_compliance_estimates_for_AB-1018_Claude_AB-1018_classification.csv'\n",
      " 3. 'Cost_compliance_estimates_for_AB-1018_Gemini_AB-1018_classification.csv'\n",
      " 4. 'Cost_compliance_estimates_for_AB-1018_Grok_AB-1018_classification.csv'\n",
      "\n",
      "============================================================\n",
      "AB1018-RELATED FILES:\n",
      "----------------------------------------\n",
      " 1. 'Cost_compliance_estimates_for_AB-1018_ChatGPT_AB-1018_classification.csv'\n",
      " 2. 'Cost_compliance_estimates_for_AB-1018_Claude_AB-1018_classification.csv'\n",
      " 3. 'Cost_compliance_estimates_for_AB-1018_Gemini_AB-1018_classification.csv'\n",
      " 4. 'Cost_compliance_estimates_for_AB-1018_Grok_AB-1018_classification.csv'\n",
      "\n",
      "============================================================\n",
      "COPY-PASTE READY FILE NAMES:\n",
      "----------------------------------------\n",
      "Use these exact names in your pandas.read_csv() calls:\n",
      "\n",
      "ChatGPT file: 'Cost_compliance_estimates_for_AB-1018_ChatGPT_AB-1018_classification.csv'\n",
      "Claude file:  'Cost_compliance_estimates_for_AB-1018_Claude_AB-1018_classification.csv'\n",
      "Gemini file:  'Cost_compliance_estimates_for_AB-1018_Gemini_AB-1018_classification.csv'\n",
      "Grok file:    'Cost_compliance_estimates_for_AB-1018_Grok_AB-1018_classification.csv'\n",
      "\n",
      "============================================================\n",
      "PATTERN SEARCH RESULTS:\n",
      "----------------------------------------\n",
      "Pattern '*AB1018*.csv': No matches\n",
      "Pattern '*AB-1018*.csv': ['Cost_compliance_estimates_for_AB-1018_ChatGPT_AB-1018_classification.csv', 'Cost_compliance_estimates_for_AB-1018_Grok_AB-1018_classification.csv', 'Cost_compliance_estimates_for_AB-1018_Claude_AB-1018_classification.csv', 'Cost_compliance_estimates_for_AB-1018_Gemini_AB-1018_classification.csv']\n",
      "Pattern '*ChatGPT*.csv': ['Cost_compliance_estimates_for_AB-1018_ChatGPT_AB-1018_classification.csv']\n",
      "Pattern '*Claude*.csv': ['Cost_compliance_estimates_for_AB-1018_Claude_AB-1018_classification.csv']\n",
      "Pattern '*Gemini*.csv': ['Cost_compliance_estimates_for_AB-1018_Gemini_AB-1018_classification.csv']\n",
      "Pattern '*Grok*.csv': ['Cost_compliance_estimates_for_AB-1018_Grok_AB-1018_classification.csv']\n",
      "\n",
      "============================================================\n",
      "SUGGESTED CODE FOR YOUR MERGER SCRIPT:\n",
      "----------------------------------------\n",
      "# Replace the file reading section in your merger script with:\n",
      "chatgpt_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_ChatGPT_AB-1018_classification.csv')\n",
      "claude_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_Claude_AB-1018_classification.csv')\n",
      "gemini_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_Gemini_AB-1018_classification.csv')\n",
      "grok_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_Grok_AB-1018_classification.csv')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def list_directory_files():\n",
    "    \"\"\"\n",
    "    List all files in the current directory and identify AB1018-related CSV files.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DIRECTORY FILE LISTING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get current working directory\n",
    "    current_dir = os.getcwd()\n",
    "    print(f\"Current directory: {current_dir}\\n\")\n",
    "    \n",
    "    # List all files in the directory\n",
    "    print(\"ALL FILES IN DIRECTORY:\")\n",
    "    print(\"-\" * 40)\n",
    "    all_files = os.listdir('.')\n",
    "    all_files.sort()  # Sort alphabetically\n",
    "    \n",
    "    for i, file in enumerate(all_files, 1):\n",
    "        print(f\"{i:2d}. {file}\")\n",
    "    \n",
    "    print(f\"\\nTotal files: {len(all_files)}\")\n",
    "    \n",
    "    # Filter for CSV files\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CSV FILES ONLY:\")\n",
    "    print(\"-\" * 40)\n",
    "    csv_files = [f for f in all_files if f.lower().endswith('.csv')]\n",
    "    \n",
    "    if csv_files:\n",
    "        for i, file in enumerate(csv_files, 1):\n",
    "            print(f\"{i:2d}. '{file}'\")\n",
    "    else:\n",
    "        print(\"No CSV files found.\")\n",
    "    \n",
    "    # Filter for AB1018-related files\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"AB1018-RELATED FILES:\")\n",
    "    print(\"-\" * 40)\n",
    "    ab1018_files = [f for f in all_files if 'AB1018' in f or 'AB-1018' in f or 'ab1018' in f.lower()]\n",
    "    \n",
    "    if ab1018_files:\n",
    "        for i, file in enumerate(ab1018_files, 1):\n",
    "            print(f\"{i:2d}. '{file}'\")\n",
    "            \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"COPY-PASTE READY FILE NAMES:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(\"Use these exact names in your pandas.read_csv() calls:\\n\")\n",
    "        \n",
    "        for file in ab1018_files:\n",
    "            if 'chatgpt' in file.lower():\n",
    "                print(f\"ChatGPT file: '{file}'\")\n",
    "            elif 'claude' in file.lower():\n",
    "                print(f\"Claude file:  '{file}'\")\n",
    "            elif 'gemini' in file.lower():\n",
    "                print(f\"Gemini file:  '{file}'\")\n",
    "            elif 'grok' in file.lower():\n",
    "                print(f\"Grok file:    '{file}'\")\n",
    "    else:\n",
    "        print(\"No AB1018-related files found.\")\n",
    "    \n",
    "    # Search for pattern variations\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PATTERN SEARCH RESULTS:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    patterns = [\n",
    "        '*AB1018*.csv',\n",
    "        '*AB-1018*.csv',\n",
    "        '*ChatGPT*.csv',\n",
    "        '*Claude*.csv',\n",
    "        '*Gemini*.csv',\n",
    "        '*Grok*.csv'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        matches = glob.glob(pattern)\n",
    "        if matches:\n",
    "            print(f\"Pattern '{pattern}': {matches}\")\n",
    "        else:\n",
    "            print(f\"Pattern '{pattern}': No matches\")\n",
    "    \n",
    "    return csv_files, ab1018_files\n",
    "\n",
    "# Run the function\n",
    "csv_files, ab1018_files = list_directory_files()\n",
    "\n",
    "# Additional helper: Generate code snippets\n",
    "if ab1018_files:\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SUGGESTED CODE FOR YOUR MERGER SCRIPT:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    file_mapping = {}\n",
    "    for file in ab1018_files:\n",
    "        if 'chatgpt' in file.lower():\n",
    "            file_mapping['chatgpt'] = file\n",
    "        elif 'claude' in file.lower():\n",
    "            file_mapping['claude'] = file\n",
    "        elif 'gemini' in file.lower():\n",
    "            file_mapping['gemini'] = file\n",
    "        elif 'grok' in file.lower():\n",
    "            file_mapping['grok'] = file\n",
    "    \n",
    "    print(\"# Replace the file reading section in your merger script with:\")\n",
    "    for llm, filename in file_mapping.items():\n",
    "        print(f\"{llm}_df = pd.read_csv('{filename}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e242551f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging AB1018 classification files...\n",
      "\n",
      "Merge completed successfully!\n",
      "Total unique NAICS codes: 98\n",
      "Final dataset shape: (98, 7)\n",
      "\n",
      "First 5 rows of merged data:\n",
      "                                       Business Type NAICS Code  \\\n",
      "0             Grok: \"Utility Providers (e.g., PG&E)\"     221100   \n",
      "1  ChatGPT: \"Energy utilities using AI for subsid...     221118   \n",
      "2                       Claude: \"Electric utilities\"     221122   \n",
      "3  ChatGPT: \"Energy utilities using AI for subsid...     221210   \n",
      "4  ChatGPT: \"Energy utilities using AI for subsid...     221310   \n",
      "\n",
      "                 NAICS Description ChatGPT Regulation Type  \\\n",
      "0                                                            \n",
      "1  Other Electric Power Generation                Deployer   \n",
      "2      Electric Power Distribution                           \n",
      "3         Natural Gas Distribution                Deployer   \n",
      "4                     Water Supply                Deployer   \n",
      "\n",
      "  Claude Regulation Type Gemini Regulation Type Grok Regulation Type  \n",
      "0                                                     Deployer rules  \n",
      "1                                                                     \n",
      "2               Deployer                                              \n",
      "3                                                                     \n",
      "4               Deployer                                              \n",
      "\n",
      "Merged data saved to: AB1018_merged_classifications.csv\n",
      "\n",
      "Summary by regulation type:\n",
      "\n",
      "ChatGPT Regulation Type:\n",
      "ChatGPT Regulation Type\n",
      "             54\n",
      "Deployer     23\n",
      "Both         16\n",
      "Developer     5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Claude Regulation Type:\n",
      "Claude Regulation Type\n",
      "             43\n",
      "Deployer     41\n",
      "Both          8\n",
      "Developer     6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Gemini Regulation Type:\n",
      "Gemini Regulation Type\n",
      "             83\n",
      "Both          8\n",
      "Deployer      6\n",
      "Developer     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Grok Regulation Type:\n",
      "Grok Regulation Type\n",
      "                   71\n",
      "Both               15\n",
      "Deployer rules      8\n",
      "Developer rules     4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def merge_ab1018_classifications():\n",
    "    \"\"\"\n",
    "    Merge AB1018 classification CSV files from ChatGPT, Claude, Gemini, and Grok.\n",
    "    Creates a consolidated table with merged business types and regulation types.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read all four CSV files\n",
    "    chatgpt_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_ChatGPT_AB-1018_classification.csv')\n",
    "    claude_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_Claude_AB-1018_classification.csv')\n",
    "    gemini_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_Gemini_AB-1018_classification.csv')\n",
    "    grok_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_Grok_AB-1018_classification.csv')\n",
    "    \n",
    "    # Clean and standardize column names\n",
    "    def clean_columns(df):\n",
    "        df.columns = df.columns.str.strip()\n",
    "        return df\n",
    "    \n",
    "    chatgpt_df = clean_columns(chatgpt_df)\n",
    "    claude_df = clean_columns(claude_df)\n",
    "    gemini_df = clean_columns(gemini_df)\n",
    "    grok_df = clean_columns(grok_df)\n",
    "    \n",
    "    # Convert NAICS Code to string for consistent joining\n",
    "    # Handle special cases in Gemini data (ranges like \"44-45\", \"31-33\")\n",
    "    chatgpt_df['NAICS Code'] = chatgpt_df['NAICS Code'].astype(str)\n",
    "    claude_df['NAICS Code'] = claude_df['NAICS Code'].astype(str)\n",
    "    gemini_df['NAICS Code'] = gemini_df['NAICS Code'].astype(str).str.strip()\n",
    "    grok_df['NAICS Code'] = grok_df['NAICS Code'].astype(str)\n",
    "    \n",
    "    # Add source identifier to each dataframe\n",
    "    chatgpt_df['Source'] = 'ChatGPT'\n",
    "    claude_df['Source'] = 'Claude'\n",
    "    gemini_df['Source'] = 'Gemini'\n",
    "    grok_df['Source'] = 'Grok'\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    all_data = pd.concat([chatgpt_df, claude_df, gemini_df, grok_df], ignore_index=True)\n",
    "    \n",
    "    # Get all unique NAICS codes\n",
    "    all_naics = all_data['NAICS Code'].unique()\n",
    "    \n",
    "    # Create the merged dataset\n",
    "    merged_rows = []\n",
    "    \n",
    "    for naics in all_naics:\n",
    "        # Filter data for this NAICS code\n",
    "        naics_data = all_data[all_data['NAICS Code'] == naics]\n",
    "        \n",
    "        # Initialize the row\n",
    "        row = {\n",
    "            'NAICS Code': naics,\n",
    "            'NAICS Description': '',\n",
    "            'Business Type': '',\n",
    "            'ChatGPT Regulation Type': '',\n",
    "            'Claude Regulation Type': '',\n",
    "            'Gemini Regulation Type': '',\n",
    "            'Grok Regulation Type': ''\n",
    "        }\n",
    "        \n",
    "        # Collect business types and regulation types by source\n",
    "        business_types = []\n",
    "        \n",
    "        for _, record in naics_data.iterrows():\n",
    "            source = record['Source']\n",
    "            business_type = str(record['Business Type']).strip() if pd.notna(record['Business Type']) else ''\n",
    "            regulation_type = str(record['Regulation Type']).strip() if pd.notna(record['Regulation Type']) else ''\n",
    "            naics_desc = str(record['NAICS Description']).strip() if pd.notna(record['NAICS Description']) else ''\n",
    "            \n",
    "            # Add to business types list\n",
    "            if business_type:\n",
    "                business_types.append(f\"{source}: \\\"{business_type}\\\"\")\n",
    "            \n",
    "            # Set regulation type for this source\n",
    "            row[f'{source} Regulation Type'] = regulation_type\n",
    "            \n",
    "            # Use the first non-empty NAICS description found\n",
    "            if not row['NAICS Description'] and naics_desc and naics_desc != 'nan':\n",
    "                row['NAICS Description'] = naics_desc\n",
    "        \n",
    "        # Join business types\n",
    "        row['Business Type'] = ' '.join(business_types)\n",
    "        \n",
    "        merged_rows.append(row)\n",
    "    \n",
    "    # Create the final dataframe\n",
    "    merged_df = pd.DataFrame(merged_rows)\n",
    "    \n",
    "    # Reorder columns as requested\n",
    "    column_order = [\n",
    "        'Business Type',\n",
    "        'NAICS Code', \n",
    "        'NAICS Description',\n",
    "        'ChatGPT Regulation Type',\n",
    "        'Claude Regulation Type',\n",
    "        'Gemini Regulation Type',\n",
    "        'Grok Regulation Type'\n",
    "    ]\n",
    "    \n",
    "    merged_df = merged_df[column_order]\n",
    "    \n",
    "    # Sort by NAICS Code for better organization\n",
    "    merged_df = merged_df.sort_values('NAICS Code').reset_index(drop=True)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Execute the merge\n",
    "print(\"Merging AB1018 classification files...\")\n",
    "result_df = merge_ab1018_classifications()\n",
    "\n",
    "print(f\"\\nMerge completed successfully!\")\n",
    "print(f\"Total unique NAICS codes: {len(result_df)}\")\n",
    "print(f\"Final dataset shape: {result_df.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of merged data:\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "print(result_df.head())\n",
    "\n",
    "# Save the merged data\n",
    "output_filename = 'AB1018_merged_classifications.csv'\n",
    "result_df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nMerged data saved to: {output_filename}\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(\"\\nSummary by regulation type:\")\n",
    "for col in ['ChatGPT Regulation Type', 'Claude Regulation Type', 'Gemini Regulation Type', 'Grok Regulation Type']:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(result_df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db94a8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging AB1018 classification files...\n",
      "\n",
      "Merge completed successfully!\n",
      "Total unique NAICS codes: 98\n",
      "Final dataset shape: (98, 7)\n",
      "\n",
      "First 5 rows of merged data:\n",
      "                                       Business Type NAICS Code  \\\n",
      "0             Grok: \"Utility Providers (e.g., PG&E)\"     221100   \n",
      "1  ChatGPT: \"Energy utilities using AI for subsid...     221118   \n",
      "2                       Claude: \"Electric utilities\"     221122   \n",
      "3  ChatGPT: \"Energy utilities using AI for subsid...     221210   \n",
      "4  ChatGPT: \"Energy utilities using AI for subsid...     221310   \n",
      "\n",
      "                 NAICS Description ChatGPT Regulation Type  \\\n",
      "0                                                            \n",
      "1  Other Electric Power Generation                Deployer   \n",
      "2      Electric Power Distribution                           \n",
      "3         Natural Gas Distribution                Deployer   \n",
      "4                     Water Supply                Deployer   \n",
      "\n",
      "  Claude Regulation Type Gemini Regulation Type Grok Regulation Type  \n",
      "0                                                     Deployer rules  \n",
      "1                                                                     \n",
      "2               Deployer                                              \n",
      "3                                                                     \n",
      "4               Deployer                                              \n",
      "\n",
      "Merged data saved to: AB1018_merged_classifications.csv\n",
      "\n",
      "Summary by regulation type:\n",
      "\n",
      "ChatGPT Regulation Type:\n",
      "ChatGPT Regulation Type\n",
      "                                                                                                                      54\n",
      "Deployer                                                                                                              21\n",
      "Both                                                                                                                  13\n",
      "Developer                                                                                                              3\n",
      "Deployer | Deployer                                                                                                    2\n",
      "Developer | Both | Developer | Developer | Developer | Both | Developer | Developer | Developer | Both | Developer     1\n",
      "Developer | Deployer | Both                                                                                            1\n",
      "Developer | Developer | Developer | Both                                                                               1\n",
      "Both | Developer | Both                                                                                                1\n",
      "Developer | Developer | Developer                                                                                      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Claude Regulation Type:\n",
      "Claude Regulation Type\n",
      "                                                                                                                                                                                                                                                                                                                                                             43\n",
      "Deployer                                                                                                                                                                                                                                                                                                                                                     36\n",
      "Both                                                                                                                                                                                                                                                                                                                                                          7\n",
      "Developer                                                                                                                                                                                                                                                                                                                                                     5\n",
      "Deployer | Deployer | Deployer                                                                                                                                                                                                                                                                                                                                3\n",
      "Deployer | Deployer                                                                                                                                                                                                                                                                                                                                           2\n",
      "Both | Both                                                                                                                                                                                                                                                                                                                                                   1\n",
      "Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer | Developer     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Gemini Regulation Type:\n",
      "Gemini Regulation Type\n",
      "                                                 83\n",
      "Both                                              7\n",
      "Deployer                                          6\n",
      "Developer | Developer | Developer | Developer     1\n",
      "Both | Both                                       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Grok Regulation Type:\n",
      "Grok Regulation Type\n",
      "                                 71\n",
      "Both                             14\n",
      "Deployer rules                    8\n",
      "Developer rules                   3\n",
      "Both | Both | Developer rules     1\n",
      "Both | Both                       1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def merge_ab1018_classifications():\n",
    "    \"\"\"\n",
    "    Merge AB1018 classification CSV files from ChatGPT, Claude, Gemini, and Grok.\n",
    "    Creates a consolidated table with merged business types and regulation types.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read all four CSV files (using exact file names from your documents)\n",
    "    chatgpt_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_ChatGPT_AB-1018_classification.csv')\n",
    "    claude_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_Claude_AB-1018_classification.csv')\n",
    "    gemini_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_Gemini_AB-1018_classification.csv')\n",
    "    grok_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_Grok_AB-1018_classification.csv')\n",
    "    \n",
    "    # Alternative: If the above file names don't work, try these variations:\n",
    "    # chatgpt_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_ChatGPT_AB1018_classification.csv')\n",
    "    # claude_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_Claude_AB1018_classification.csv')\n",
    "    # gemini_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_Gemini_AB1018_classification.csv')\n",
    "    # grok_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_Grok_AB1018_classification.csv')\n",
    "    \n",
    "    # Clean and standardize column names\n",
    "    def clean_columns(df):\n",
    "        df.columns = df.columns.str.strip()\n",
    "        return df\n",
    "    \n",
    "    chatgpt_df = clean_columns(chatgpt_df)\n",
    "    claude_df = clean_columns(claude_df)\n",
    "    gemini_df = clean_columns(gemini_df)\n",
    "    grok_df = clean_columns(grok_df)\n",
    "    \n",
    "    # Convert NAICS Code to string for consistent joining\n",
    "    # Handle special cases in Gemini data (ranges like \"44-45\", \"31-33\")\n",
    "    chatgpt_df['NAICS Code'] = chatgpt_df['NAICS Code'].astype(str)\n",
    "    claude_df['NAICS Code'] = claude_df['NAICS Code'].astype(str)\n",
    "    gemini_df['NAICS Code'] = gemini_df['NAICS Code'].astype(str).str.strip()\n",
    "    grok_df['NAICS Code'] = grok_df['NAICS Code'].astype(str)\n",
    "    \n",
    "    # Add source identifier to each dataframe\n",
    "    chatgpt_df['Source'] = 'ChatGPT'\n",
    "    claude_df['Source'] = 'Claude'\n",
    "    gemini_df['Source'] = 'Gemini'\n",
    "    grok_df['Source'] = 'Grok'\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    all_data = pd.concat([chatgpt_df, claude_df, gemini_df, grok_df], ignore_index=True)\n",
    "    \n",
    "    # Create a comprehensive merge that preserves all unique combinations\n",
    "    # First, let's create a lookup table for each source\n",
    "    source_data = {}\n",
    "    \n",
    "    for source in ['ChatGPT', 'Claude', 'Gemini', 'Grok']:\n",
    "        source_df = all_data[all_data['Source'] == source].copy()\n",
    "        # Group by NAICS code and aggregate business types and regulation types\n",
    "        grouped = source_df.groupby('NAICS Code').agg({\n",
    "            'Business Type': lambda x: ' | '.join([str(bt).strip() for bt in x if pd.notna(bt) and str(bt).strip()]),\n",
    "            'Regulation Type': lambda x: ' | '.join([str(rt).strip() for rt in x if pd.notna(rt) and str(rt).strip()]),\n",
    "            'NAICS Description': 'first'  # Take the first non-null description\n",
    "        }).reset_index()\n",
    "        source_data[source] = grouped\n",
    "    \n",
    "    # Get all unique NAICS codes across all sources\n",
    "    all_naics = set()\n",
    "    for source_df in source_data.values():\n",
    "        all_naics.update(source_df['NAICS Code'].tolist())\n",
    "    \n",
    "    # Create the merged dataset\n",
    "    merged_rows = []\n",
    "    \n",
    "    for naics in all_naics:\n",
    "        # Initialize the row\n",
    "        row = {\n",
    "            'NAICS Code': naics,\n",
    "            'NAICS Description': '',\n",
    "            'Business Type': '',\n",
    "            'ChatGPT Regulation Type': '',\n",
    "            'Claude Regulation Type': '',\n",
    "            'Gemini Regulation Type': '',\n",
    "            'Grok Regulation Type': ''\n",
    "        }\n",
    "        \n",
    "        # Collect business types and regulation types by source\n",
    "        business_type_parts = []\n",
    "        \n",
    "        for source in ['ChatGPT', 'Claude', 'Gemini', 'Grok']:\n",
    "            source_df = source_data[source]\n",
    "            naics_records = source_df[source_df['NAICS Code'] == naics]\n",
    "            \n",
    "            if not naics_records.empty:\n",
    "                record = naics_records.iloc[0]  # Take the first (and should be only) record\n",
    "                \n",
    "                business_type = str(record['Business Type']).strip() if pd.notna(record['Business Type']) else ''\n",
    "                regulation_type = str(record['Regulation Type']).strip() if pd.notna(record['Regulation Type']) else ''\n",
    "                naics_desc = str(record['NAICS Description']).strip() if pd.notna(record['NAICS Description']) else ''\n",
    "                \n",
    "                # Add to business types list if not empty\n",
    "                if business_type and business_type != 'nan':\n",
    "                    business_type_parts.append(f\"{source}: \\\"{business_type}\\\"\")\n",
    "                \n",
    "                # Set regulation type for this source\n",
    "                if regulation_type and regulation_type != 'nan':\n",
    "                    row[f'{source} Regulation Type'] = regulation_type\n",
    "                \n",
    "                # Use the first non-empty NAICS description found\n",
    "                if not row['NAICS Description'] and naics_desc and naics_desc != 'nan':\n",
    "                    row['NAICS Description'] = naics_desc\n",
    "        \n",
    "        # Join business types\n",
    "        row['Business Type'] = ' '.join(business_type_parts)\n",
    "        \n",
    "        merged_rows.append(row)\n",
    "    \n",
    "    # Create the final dataframe\n",
    "    merged_df = pd.DataFrame(merged_rows)\n",
    "    \n",
    "    # Reorder columns as requested\n",
    "    column_order = [\n",
    "        'Business Type',\n",
    "        'NAICS Code', \n",
    "        'NAICS Description',\n",
    "        'ChatGPT Regulation Type',\n",
    "        'Claude Regulation Type',\n",
    "        'Gemini Regulation Type',\n",
    "        'Grok Regulation Type'\n",
    "    ]\n",
    "    \n",
    "    merged_df = merged_df[column_order]\n",
    "    \n",
    "    # Sort by NAICS Code for better organization\n",
    "    merged_df = merged_df.sort_values('NAICS Code').reset_index(drop=True)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Execute the merge\n",
    "print(\"Merging AB1018 classification files...\")\n",
    "result_df = merge_ab1018_classifications()\n",
    "\n",
    "print(f\"\\nMerge completed successfully!\")\n",
    "print(f\"Total unique NAICS codes: {len(result_df)}\")\n",
    "print(f\"Final dataset shape: {result_df.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 5 rows of merged data:\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "print(result_df.head())\n",
    "\n",
    "# Save the merged data\n",
    "output_filename = 'AB1018_merged_classifications.csv'\n",
    "result_df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nMerged data saved to: {output_filename}\")\n",
    "\n",
    "# Show summary statistics\n",
    "print(\"\\nSummary by regulation type:\")\n",
    "for col in ['ChatGPT Regulation Type', 'Claude Regulation Type', 'Gemini Regulation Type', 'Grok Regulation Type']:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(result_df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a38500f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved merged file to AB1018_LLMs_merged.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAICS</th>\n",
       "      <th>Business Type</th>\n",
       "      <th>ChatGPT Regulation Type</th>\n",
       "      <th>Claude Regulation Type</th>\n",
       "      <th>Grok Regulation Type</th>\n",
       "      <th>Gemini Regulation Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221100</td>\n",
       "      <td>Grok: \"Utility Providers (e.g., PG&amp;E)\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deployer rules</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221118</td>\n",
       "      <td>ChatGPT: \"Energy utilities using AI for subsid...</td>\n",
       "      <td>Deployer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221122</td>\n",
       "      <td>Claude: \"Electric utilities\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deployer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221210</td>\n",
       "      <td>ChatGPT: \"Energy utilities using AI for subsid...</td>\n",
       "      <td>Deployer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221310</td>\n",
       "      <td>ChatGPT: \"Energy utilities using AI for subsid...</td>\n",
       "      <td>Deployer</td>\n",
       "      <td>Deployer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>221320</td>\n",
       "      <td>Grok: \"Water and Waste Management Firms (e.g.,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deployer rules</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>31-33</td>\n",
       "      <td>Gemini: \"Large Corporations (as employers)\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deployer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>325412</td>\n",
       "      <td>Claude: \"Pharmaceutical companies using AI\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deployer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>334118</td>\n",
       "      <td>Claude: \"Voting system manufacturers\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Developer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>334519</td>\n",
       "      <td>Grok: \"Election Tech Providers (e.g., Dominion...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Developer rules</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>44-45</td>\n",
       "      <td>Gemini: \"Large Corporations (as employers)\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deployer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>452111</td>\n",
       "      <td>Claude: \"Retail stores using AI\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deployer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>454110</td>\n",
       "      <td>Grok: \"E-commerce Platforms (e.g., Amazon)\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Both</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>485111</td>\n",
       "      <td>Claude: \"Public transit agencies\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Deployer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>485310</td>\n",
       "      <td>ChatGPT: \"Transit agencies and rideshare platf...</td>\n",
       "      <td>Both</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Both</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>485999</td>\n",
       "      <td>ChatGPT: \"Transit agencies and rideshare platf...</td>\n",
       "      <td>Both</td>\n",
       "      <td>Both</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>485999</td>\n",
       "      <td>ChatGPT: \"Transit agencies and rideshare platf...</td>\n",
       "      <td>Both</td>\n",
       "      <td>Both</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>488999</td>\n",
       "      <td>Gemini: \"Ride-Sharing &amp; Delivery Platforms\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Both</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>492110</td>\n",
       "      <td>Claude: \"Delivery companies\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Both</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>511210</td>\n",
       "      <td>ChatGPT: \"HR tech vendors (AI hiring, scheduli...</td>\n",
       "      <td>Developer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     NAICS                                      Business Type  \\\n",
       "0   221100             Grok: \"Utility Providers (e.g., PG&E)\"   \n",
       "1   221118  ChatGPT: \"Energy utilities using AI for subsid...   \n",
       "2   221122                       Claude: \"Electric utilities\"   \n",
       "3   221210  ChatGPT: \"Energy utilities using AI for subsid...   \n",
       "4   221310  ChatGPT: \"Energy utilities using AI for subsid...   \n",
       "5   221320  Grok: \"Water and Waste Management Firms (e.g.,...   \n",
       "6    31-33        Gemini: \"Large Corporations (as employers)\"   \n",
       "7   325412        Claude: \"Pharmaceutical companies using AI\"   \n",
       "8   334118              Claude: \"Voting system manufacturers\"   \n",
       "9   334519  Grok: \"Election Tech Providers (e.g., Dominion...   \n",
       "10   44-45        Gemini: \"Large Corporations (as employers)\"   \n",
       "11  452111                   Claude: \"Retail stores using AI\"   \n",
       "12  454110        Grok: \"E-commerce Platforms (e.g., Amazon)\"   \n",
       "13  485111                  Claude: \"Public transit agencies\"   \n",
       "14  485310  ChatGPT: \"Transit agencies and rideshare platf...   \n",
       "15  485999  ChatGPT: \"Transit agencies and rideshare platf...   \n",
       "16  485999  ChatGPT: \"Transit agencies and rideshare platf...   \n",
       "17  488999        Gemini: \"Ride-Sharing & Delivery Platforms\"   \n",
       "18  492110                       Claude: \"Delivery companies\"   \n",
       "19  511210  ChatGPT: \"HR tech vendors (AI hiring, scheduli...   \n",
       "\n",
       "   ChatGPT Regulation Type Claude Regulation Type Grok Regulation Type  \\\n",
       "0                      NaN                    NaN       Deployer rules   \n",
       "1                 Deployer                    NaN                  NaN   \n",
       "2                      NaN               Deployer                  NaN   \n",
       "3                 Deployer                    NaN                  NaN   \n",
       "4                 Deployer               Deployer                  NaN   \n",
       "5                      NaN                    NaN       Deployer rules   \n",
       "6                      NaN                    NaN                  NaN   \n",
       "7                      NaN               Deployer                  NaN   \n",
       "8                      NaN              Developer                  NaN   \n",
       "9                      NaN                    NaN      Developer rules   \n",
       "10                     NaN                    NaN                  NaN   \n",
       "11                     NaN               Deployer                  NaN   \n",
       "12                     NaN                    NaN                 Both   \n",
       "13                     NaN               Deployer                  NaN   \n",
       "14                    Both                    NaN                 Both   \n",
       "15                    Both                   Both                  NaN   \n",
       "16                    Both                   Both                  NaN   \n",
       "17                     NaN                    NaN                  NaN   \n",
       "18                     NaN                   Both                  NaN   \n",
       "19               Developer                    NaN                  NaN   \n",
       "\n",
       "   Gemini Regulation Type  \n",
       "0                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  \n",
       "5                     NaN  \n",
       "6                Deployer  \n",
       "7                     NaN  \n",
       "8                     NaN  \n",
       "9                     NaN  \n",
       "10               Deployer  \n",
       "11                    NaN  \n",
       "12                    NaN  \n",
       "13                    NaN  \n",
       "14                    NaN  \n",
       "15                    NaN  \n",
       "16                    NaN  \n",
       "17                   Both  \n",
       "18                    NaN  \n",
       "19                    NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# ---- Inputs: update paths if needed ----\n",
    "paths = {\n",
    "    \"ChatGPT\": \"Cost_compliance_estimates_for_AB-1018_ChatGPT_AB-1018_classification.csv\",\n",
    "    \"Claude\":  \"Cost_compliance_estimates_for_AB-1018_Claude_AB-1018_classification.csv\",\n",
    "    \"Grok\":    \"Cost_compliance_estimates_for_AB-1018_Grok_AB-1018_classification.csv\",\n",
    "    \"Gemini\":  \"Cost_compliance_estimates_for_AB-1018_Gemini_AB-1018_classification.csv\",\n",
    "}\n",
    "\n",
    "# ---- Helper functions ----\n",
    "def find_col(cols, candidates):\n",
    "    lower = {c.lower(): c for c in cols}\n",
    "    for cand in candidates:\n",
    "        if cand.lower() in lower:\n",
    "            return lower[cand.lower()]\n",
    "    return None\n",
    "\n",
    "def normalize_naics(value):\n",
    "    if pd.isna(value):\n",
    "        return None\n",
    "    s = str(value).strip()\n",
    "    if re.fullmatch(r\"\\d{6}\", s):\n",
    "        return s\n",
    "    if re.fullmatch(r\"\\d{2}\\s*-\\s*\\d{2}\", s):\n",
    "        return s.replace(\" \", \"\")\n",
    "    m6 = re.search(r\"(\\d{6})\", s)\n",
    "    if m6:\n",
    "        return m6.group(1)\n",
    "    mrange = re.search(r\"(\\d{2})\\s*-\\s*(\\d{2})\", s)\n",
    "    if mrange:\n",
    "        return f\"{mrange.group(1)}-{mrange.group(2)}\"\n",
    "    return s\n",
    "\n",
    "def load_and_standardize(model_name, path):\n",
    "    df = pd.read_csv(path, dtype=str, keep_default_na=False).replace({\"\": pd.NA})\n",
    "    original_cols = list(df.columns)\n",
    "\n",
    "    naics_col = find_col(original_cols, [\"NAICS\", \"NAICS Code\", \"Naics\", \"naics\"])\n",
    "    reg_col = find_col(original_cols, [\"Regulation Type\", \"Regulated Type\", \"Regulation\", \"Regulated\"])\n",
    "    business_candidates = [\"Business Type\", \"Business\", \"Examples\", \"Example Businesses\", \"Description\", \"Details\", \"Notes\", \"Label\", \"Category\"]\n",
    "    business_col = find_col(original_cols, business_candidates)\n",
    "\n",
    "    if business_col is None:\n",
    "        exclude = {naics_col, reg_col}\n",
    "        textish = [c for c in original_cols if c not in exclude and c is not None]\n",
    "        business_col = textish[0] if textish else None\n",
    "\n",
    "    slim = pd.DataFrame()\n",
    "    if naics_col is None:\n",
    "        slim[\"NAICS\"] = [f\"{model_name}_row_{i}\" for i in range(len(df))]\n",
    "    else:\n",
    "        slim[\"NAICS\"] = df[naics_col].map(normalize_naics)\n",
    "\n",
    "    slim[f\"{model_name}_business_text\"] = df[business_col] if business_col else pd.NA\n",
    "    slim[f\"{model_name} Regulation Type\"] = df[reg_col] if reg_col else pd.NA\n",
    "\n",
    "    return slim\n",
    "\n",
    "# ---- Load all four ----\n",
    "frames = [load_and_standardize(model, p) for model, p in paths.items()]\n",
    "\n",
    "# ---- Merge ----\n",
    "merged = reduce(lambda l, r: pd.merge(l, r, on=\"NAICS\", how=\"outer\"), frames)\n",
    "\n",
    "def build_business_type(row):\n",
    "    parts = []\n",
    "    for model in [\"ChatGPT\", \"Claude\", \"Grok\", \"Gemini\"]:\n",
    "        txt = row.get(f\"{model}_business_text\", pd.NA)\n",
    "        if pd.notna(txt) and str(txt).strip():\n",
    "            parts.append(f'{model}: \"{str(txt).strip()}\"')\n",
    "    return \" ; \".join(parts) if parts else pd.NA\n",
    "\n",
    "merged[\"Business Type\"] = merged.apply(build_business_type, axis=1)\n",
    "\n",
    "final_cols = [\n",
    "    \"NAICS\",\n",
    "    \"Business Type\",\n",
    "    \"ChatGPT Regulation Type\",\n",
    "    \"Claude Regulation Type\",\n",
    "    \"Grok Regulation Type\",\n",
    "    \"Gemini Regulation Type\",\n",
    "]\n",
    "for c in final_cols:\n",
    "    if c not in merged.columns:\n",
    "        merged[c] = pd.NA\n",
    "\n",
    "final = merged[final_cols].sort_values(by=[\"NAICS\"]).reset_index(drop=True)\n",
    "\n",
    "# ---- Save and inspect ----\n",
    "out_path = \"AB1018_LLMs_merged.csv\"\n",
    "final.to_csv(out_path, index=False)\n",
    "\n",
    "print(f\"Saved merged file to {out_path}\")\n",
    "final.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35d61e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique NAICS codes: 98\n",
      "Original dataset sizes - ChatGPT: 65, Claude: 92, Gemini: 19, Grok: 30\n",
      "\n",
      "First few rows of merged data:\n",
      "                                        Business Type NAICS Code  \\\n",
      "12             Grok: \"Utility Providers (e.g., PG&E)\"     221100   \n",
      "56  ChatGPT: \"Energy utilities using AI for subsid...     221118   \n",
      "84                       Claude: \"Electric utilities\"     221122   \n",
      "8   ChatGPT: \"Energy utilities using AI for subsid...     221210   \n",
      "73  ChatGPT: \"Energy utilities using AI for subsid...     221310   \n",
      "\n",
      "                  NAICS Description ChatGPT_Regulation_Type  \\\n",
      "12                              NaN                     NaN   \n",
      "56  Other Electric Power Generation                Deployer   \n",
      "84      Electric Power Distribution                     NaN   \n",
      "8          Natural Gas Distribution                Deployer   \n",
      "73                     Water Supply                Deployer   \n",
      "\n",
      "   Claude_Regulation_Type Gemini_Regulation_Type Grok_Regulation_Type  \n",
      "12                    NaN                    NaN       Deployer rules  \n",
      "56                    NaN                    NaN                  NaN  \n",
      "84               Deployer                    NaN                  NaN  \n",
      "8                     NaN                    NaN                  NaN  \n",
      "73               Deployer                    NaN                  NaN  \n",
      "\n",
      "Merged data saved as 'AB1018_Merged_Classification.csv'\n",
      "\n",
      "Regulation Type Distribution:\n",
      "ChatGPT_Regulation_Type:\n",
      "ChatGPT_Regulation_Type\n",
      "NaN          54\n",
      "Deployer     23\n",
      "Both         14\n",
      "Developer     7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Claude_Regulation_Type:\n",
      "Claude_Regulation_Type\n",
      "NaN          43\n",
      "Deployer     41\n",
      "Both          8\n",
      "Developer     6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Gemini_Regulation_Type:\n",
      "Gemini_Regulation_Type\n",
      "NaN          83\n",
      "Both          8\n",
      "Deployer      6\n",
      "Developer     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Grok_Regulation_Type:\n",
      "Grok_Regulation_Type\n",
      "NaN                71\n",
      "Both               16\n",
      "Deployer rules      8\n",
      "Developer rules     3\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the four CSV files\n",
    "chatgpt_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_ChatGPT_AB-1018_classification.csv')\n",
    "claude_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_Claude_AB-1018_classification.csv')\n",
    "gemini_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_Gemini_AB-1018_classification.csv')\n",
    "grok_df = pd.read_csv('Cost_compliance_estimates_for_AB-1018_Grok_AB-1018_classification.csv')\n",
    "\n",
    "# Clean headers by stripping whitespace\n",
    "for df in [chatgpt_df, claude_df, gemini_df, grok_df]:\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "# Convert NAICS Code to string for consistent joining (handling the range codes in Gemini)\n",
    "chatgpt_df['NAICS Code'] = chatgpt_df['NAICS Code'].astype(str)\n",
    "claude_df['NAICS Code'] = claude_df['NAICS Code'].astype(str)\n",
    "gemini_df['NAICS Code'] = gemini_df['NAICS Code'].astype(str).str.strip()\n",
    "grok_df['NAICS Code'] = grok_df['NAICS Code'].astype(str)\n",
    "\n",
    "# Create a function to format business type entries\n",
    "def format_business_type(row, llm_name):\n",
    "    if pd.isna(row['Business Type']) or row['Business Type'] == '':\n",
    "        return f'{llm_name}: \"\"'\n",
    "    return f'{llm_name}: \"{row[\"Business Type\"]}\"'\n",
    "\n",
    "# Get all unique NAICS codes from all datasets\n",
    "all_naics = set()\n",
    "all_naics.update(chatgpt_df['NAICS Code'].tolist())\n",
    "all_naics.update(claude_df['NAICS Code'].tolist()) \n",
    "all_naics.update(gemini_df['NAICS Code'].tolist())\n",
    "all_naics.update(grok_df['NAICS Code'].tolist())\n",
    "\n",
    "# Create the master dataframe\n",
    "merged_data = []\n",
    "\n",
    "for naics in all_naics:\n",
    "    # Get entries from each dataset for this NAICS code\n",
    "    chatgpt_entries = chatgpt_df[chatgpt_df['NAICS Code'] == naics]\n",
    "    claude_entries = claude_df[claude_df['NAICS Code'] == naics]\n",
    "    gemini_entries = gemini_df[gemini_df['NAICS Code'] == naics]\n",
    "    grok_entries = grok_df[grok_df['NAICS Code'] == naics]\n",
    "    \n",
    "    # Collect business type descriptions\n",
    "    business_types = []\n",
    "    \n",
    "    # Add ChatGPT entries\n",
    "    for _, row in chatgpt_entries.iterrows():\n",
    "        business_types.append(format_business_type(row, 'ChatGPT'))\n",
    "    \n",
    "    # Add Claude entries  \n",
    "    for _, row in claude_entries.iterrows():\n",
    "        business_types.append(format_business_type(row, 'Claude'))\n",
    "        \n",
    "    # Add Grok entries\n",
    "    for _, row in grok_entries.iterrows():\n",
    "        business_types.append(format_business_type(row, 'Grok'))\n",
    "        \n",
    "    # Add Gemini entries\n",
    "    for _, row in gemini_entries.iterrows():\n",
    "        business_types.append(format_business_type(row, 'Gemini'))\n",
    "    \n",
    "    # Combine all business type descriptions\n",
    "    combined_business_type = ' '.join(business_types) if business_types else ''\n",
    "    \n",
    "    # Get regulation types (take the first non-null value for each LLM)\n",
    "    chatgpt_reg_type = chatgpt_entries['Regulation Type'].iloc[0] if not chatgpt_entries.empty else np.nan\n",
    "    claude_reg_type = claude_entries['Regulation Type'].iloc[0] if not claude_entries.empty else np.nan\n",
    "    gemini_reg_type = gemini_entries['Regulation Type'].iloc[0] if not gemini_entries.empty else np.nan\n",
    "    grok_reg_type = grok_entries['Regulation Type'].iloc[0] if not grok_entries.empty else np.nan\n",
    "    \n",
    "    # Get NAICS description (prefer the first non-null description found)\n",
    "    naics_desc = np.nan\n",
    "    for entries in [chatgpt_entries, claude_entries, gemini_entries, grok_entries]:\n",
    "        if not entries.empty and not pd.isna(entries['NAICS Description'].iloc[0]):\n",
    "            naics_desc = entries['NAICS Description'].iloc[0]\n",
    "            break\n",
    "    \n",
    "    merged_data.append({\n",
    "        'NAICS Code': naics,\n",
    "        'NAICS Description': naics_desc,\n",
    "        'Business Type': combined_business_type,\n",
    "        'ChatGPT_Regulation_Type': chatgpt_reg_type,\n",
    "        'Claude_Regulation_Type': claude_reg_type,\n",
    "        'Gemini_Regulation_Type': gemini_reg_type,\n",
    "        'Grok_Regulation_Type': grok_reg_type\n",
    "    })\n",
    "\n",
    "# Create the final dataframe\n",
    "final_df = pd.DataFrame(merged_data)\n",
    "\n",
    "# Reorder columns as requested\n",
    "column_order = [\n",
    "    'Business Type', \n",
    "    'NAICS Code', \n",
    "    'NAICS Description',\n",
    "    'ChatGPT_Regulation_Type', \n",
    "    'Claude_Regulation_Type', \n",
    "    'Gemini_Regulation_Type', \n",
    "    'Grok_Regulation_Type'\n",
    "]\n",
    "\n",
    "final_df = final_df[column_order]\n",
    "\n",
    "# Sort by NAICS Code for better organization\n",
    "final_df = final_df.sort_values('NAICS Code')\n",
    "\n",
    "# Display basic info about the merged dataset\n",
    "print(f\"Total unique NAICS codes: {len(final_df)}\")\n",
    "print(f\"Original dataset sizes - ChatGPT: {len(chatgpt_df)}, Claude: {len(claude_df)}, Gemini: {len(gemini_df)}, Grok: {len(grok_df)}\")\n",
    "print(\"\\nFirst few rows of merged data:\")\n",
    "print(final_df.head())\n",
    "\n",
    "# Save the merged dataset\n",
    "final_df.to_csv('AB1018_Merged_Classification.csv', index=False)\n",
    "print(\"\\nMerged data saved as 'AB1018_Merged_Classification.csv'\")\n",
    "\n",
    "# Show some statistics\n",
    "print(f\"\\nRegulation Type Distribution:\")\n",
    "for col in ['ChatGPT_Regulation_Type', 'Claude_Regulation_Type', 'Gemini_Regulation_Type', 'Grok_Regulation_Type']:\n",
    "    print(f\"{col}:\")\n",
    "    print(final_df[col].value_counts(dropna=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88429477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats file has 105 rows with valid NAICS codes\n",
      "Classification file has 98 rows with valid NAICS codes\n",
      "\n",
      "Found matches for 96 out of 98 classification rows\n",
      "\n",
      "Sample of NAICS matching:\n",
      "Classification: 221100 -> Stats: 22\n",
      "Classification: 221118 -> Stats: 22\n",
      "Classification: 221122 -> Stats: 22\n",
      "Classification: 221210 -> Stats: 22\n",
      "Classification: 221310 -> Stats: 22\n",
      "Classification: 221320 -> Stats: 22\n",
      "Classification: 31-33 -> Stats: 31 - 33\n",
      "Classification: 325412 -> Stats: 325\n",
      "Classification: 334118 -> Stats: 334\n",
      "Classification: 334519 -> Stats: 334\n",
      "\n",
      "Final merged dataset has 96 rows\n",
      "\n",
      "First few rows of merged data:\n",
      "  Original_Classification_NAICS Matched_Stats_NAICS   Industry   Total\n",
      "0                        221100                  22  Utilities  67,126\n",
      "1                        221118                  22  Utilities  67,126\n",
      "2                        221122                  22  Utilities  67,126\n",
      "3                        221210                  22  Utilities  67,126\n",
      "4                        221310                  22  Utilities  67,126\n",
      "\n",
      "Merged data saved as 'AB1018_Classification_with_Stats.csv'\n",
      "\n",
      "Matching Statistics:\n",
      "Classification entries matched: 96\n",
      "Classification entries unmatched: 2\n",
      "\n",
      "Unmatched Classification NAICS codes:\n",
      "['452111' '454110']\n",
      "\n",
      "Matching Granularity Distribution:\n",
      "   Original_Length  Matched_Length  Count\n",
      "0                4               3      2\n",
      "1                5               7      2\n",
      "2                6               2     18\n",
      "3                6               3     74\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV files\n",
    "stats_df = pd.read_csv('2B-24-3-FINAL.csv')\n",
    "classification_df = pd.read_csv('AB1018_Merged_Classification.csv')\n",
    "\n",
    "# Clean the NAICS codes\n",
    "stats_df['NAICS'] = stats_df['NAICS'].astype(str).str.strip()\n",
    "classification_df['NAICS Code'] = classification_df['NAICS Code'].astype(str).str.strip()\n",
    "\n",
    "# Remove empty/null NAICS codes\n",
    "stats_df = stats_df[stats_df['NAICS'].notna() & (stats_df['NAICS'] != '') & (stats_df['NAICS'] != 'nan')]\n",
    "classification_df = classification_df[classification_df['NAICS Code'].notna() & (classification_df['NAICS Code'] != '') & (classification_df['NAICS Code'] != 'nan')]\n",
    "\n",
    "print(f\"Stats file has {len(stats_df)} rows with valid NAICS codes\")\n",
    "print(f\"Classification file has {len(classification_df)} rows with valid NAICS codes\")\n",
    "\n",
    "def normalize_naics(naics_code):\n",
    "    \"\"\"Normalize NAICS codes by removing spaces and standardizing format\"\"\"\n",
    "    if pd.isna(naics_code):\n",
    "        return ''\n",
    "    naics_str = str(naics_code).strip()\n",
    "    # Handle ranges like \"31 - 33\" -> \"31-33\"\n",
    "    naics_str = naics_str.replace(' - ', '-').replace(' -', '-').replace('- ', '-')\n",
    "    return naics_str\n",
    "\n",
    "def find_best_match(classification_naics, stats_naics_list):\n",
    "    \"\"\"\n",
    "    Find the best matching NAICS code from stats data for a given classification NAICS.\n",
    "    Returns the most specific (longest) match available.\n",
    "    \"\"\"\n",
    "    classification_naics = normalize_naics(classification_naics)\n",
    "    \n",
    "    # Handle special range cases\n",
    "    if classification_naics == '31-33':\n",
    "        # Look for exact match first, then 3-digit matches, then 2-digit\n",
    "        candidates = ['31-33', '31 - 33', '311', '312', '313', '314', '315', '316', '321', '322', '323', '324', '325', '326', '327', '331', '332', '333', '334', '335', '336', '337', '339']\n",
    "    elif classification_naics == '44-45':\n",
    "        candidates = ['44-45', '44 - 45', '441', '444', '445', '449', '455', '456', '457', '458', '459']\n",
    "    else:\n",
    "        # For regular NAICS codes, create a hierarchy of potential matches\n",
    "        candidates = []\n",
    "        \n",
    "        # If it's a 6-digit code, try progressively shorter versions\n",
    "        if len(classification_naics) == 6:\n",
    "            candidates = [\n",
    "                classification_naics,          # 6-digit exact\n",
    "                classification_naics[:5],      # 5-digit\n",
    "                classification_naics[:4],      # 4-digit\n",
    "                classification_naics[:3],      # 3-digit\n",
    "                classification_naics[:2]       # 2-digit\n",
    "            ]\n",
    "        elif len(classification_naics) == 4:\n",
    "            candidates = [\n",
    "                classification_naics,          # 4-digit exact\n",
    "                classification_naics[:3],      # 3-digit\n",
    "                classification_naics[:2]       # 2-digit\n",
    "            ]\n",
    "        else:\n",
    "            candidates = [classification_naics]\n",
    "    \n",
    "    # Find the best match (most specific available)\n",
    "    for candidate in candidates:\n",
    "        normalized_candidate = normalize_naics(candidate)\n",
    "        for stats_naics in stats_naics_list:\n",
    "            normalized_stats = normalize_naics(stats_naics)\n",
    "            if normalized_candidate == normalized_stats:\n",
    "                return stats_naics\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Get list of all NAICS codes from stats data\n",
    "stats_naics_list = stats_df['NAICS'].unique().tolist()\n",
    "\n",
    "# Create mapping for each classification NAICS to best matching stats NAICS\n",
    "classification_df['Matched_Stats_NAICS'] = classification_df['NAICS Code'].apply(\n",
    "    lambda x: find_best_match(x, stats_naics_list)\n",
    ")\n",
    "\n",
    "# Filter to only rows that have matches\n",
    "matched_classification = classification_df[classification_df['Matched_Stats_NAICS'].notna()].copy()\n",
    "\n",
    "print(f\"\\nFound matches for {len(matched_classification)} out of {len(classification_df)} classification rows\")\n",
    "\n",
    "# Show the matching pairs\n",
    "print(\"\\nSample of NAICS matching:\")\n",
    "sample_matches = matched_classification[['NAICS Code', 'Matched_Stats_NAICS']].head(10)\n",
    "for _, row in sample_matches.iterrows():\n",
    "    print(f\"Classification: {row['NAICS Code']} -> Stats: {row['Matched_Stats_NAICS']}\")\n",
    "\n",
    "# Merge the dataframes\n",
    "merged_df = pd.merge(\n",
    "    matched_classification,\n",
    "    stats_df,\n",
    "    left_on='Matched_Stats_NAICS',\n",
    "    right_on='NAICS',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Clean up columns - remove duplicate NAICS columns and rename for clarity\n",
    "merged_df = merged_df.drop(['Matched_Stats_NAICS'], axis=1)\n",
    "merged_df = merged_df.rename(columns={\n",
    "    'NAICS Code': 'Original_Classification_NAICS',\n",
    "    'NAICS': 'Matched_Stats_NAICS'\n",
    "})\n",
    "\n",
    "# Reorder columns for better readability\n",
    "column_order = [\n",
    "    'Original_Classification_NAICS',\n",
    "    'Matched_Stats_NAICS', \n",
    "    'NAICS Description',\n",
    "    'Industry',\n",
    "    'Business Type',\n",
    "    'ChatGPT_Regulation_Type',\n",
    "    'Claude_Regulation_Type', \n",
    "    'Gemini_Regulation_Type',\n",
    "    'Grok_Regulation_Type',\n",
    "    'Total',\n",
    "    '0-4',\n",
    "    '5-9', \n",
    "    '10-19',\n",
    "    '20-49',\n",
    "    '50-99',\n",
    "    '100-249',\n",
    "    '250-499',\n",
    "    '500-999'\n",
    "]\n",
    "\n",
    "merged_df = merged_df[column_order]\n",
    "\n",
    "# Sort by the matched NAICS code\n",
    "merged_df = merged_df.sort_values('Matched_Stats_NAICS')\n",
    "\n",
    "print(f\"\\nFinal merged dataset has {len(merged_df)} rows\")\n",
    "print(\"\\nFirst few rows of merged data:\")\n",
    "print(merged_df[['Original_Classification_NAICS', 'Matched_Stats_NAICS', 'Industry', 'Total']].head())\n",
    "\n",
    "# Save the merged dataset\n",
    "merged_df.to_csv('AB1018_Classification_with_Stats.csv', index=False)\n",
    "print(f\"\\nMerged data saved as 'AB1018_Classification_with_Stats.csv'\")\n",
    "\n",
    "# Show some statistics about the matching\n",
    "print(f\"\\nMatching Statistics:\")\n",
    "print(f\"Classification entries matched: {len(merged_df)}\")\n",
    "print(f\"Classification entries unmatched: {len(classification_df) - len(matched_classification)}\")\n",
    "\n",
    "# Show which classification NAICS couldn't be matched\n",
    "unmatched = classification_df[classification_df['Matched_Stats_NAICS'].isna()]\n",
    "if len(unmatched) > 0:\n",
    "    print(f\"\\nUnmatched Classification NAICS codes:\")\n",
    "    print(unmatched['NAICS Code'].unique())\n",
    "\n",
    "# Show matching granularity distribution\n",
    "print(f\"\\nMatching Granularity Distribution:\")\n",
    "matched_classification['Original_Length'] = matched_classification['NAICS Code'].str.len()\n",
    "matched_classification['Matched_Length'] = matched_classification['Matched_Stats_NAICS'].str.len()\n",
    "granularity_stats = matched_classification.groupby(['Original_Length', 'Matched_Length']).size().reset_index(name='Count')\n",
    "print(granularity_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a299021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
